# Digitraffic Data Pipeline with Docker, Kafka, and PySpark (Delta Lake)

## Overview

This project is a **Docker-based data pipeline** designed to fetch **live train data** from the [Digitraffic API](https://rata.digitraffic.fi/), publish it to a Kafka topic, and process the data using **PySpark with Delta Lake**.

**Key Features:**

- **Automated Data Ingestion**: A Python service fetches train data from the API and sends it to Kafka
- **Data Processing with PySpark & Delta Lake**: A Jupyter Notebook service consumes the Kafka data and processes it using PySpark and Delta tables
- **Batch Processing with Future Real-Time Capabilities**: The pipeline is currently designed for batch processing but can be modified to support streaming
- **Docker-Containerized Architecture**: The project runs three services within Docker:
  1. **Data Fetcher** (Python script) ‚Äì retrieves data from the API and publishes to Kafka
  2. **Kafka Broker** ‚Äì manages the message queue
  3. **PySpark Notebook** ‚Äì processes the ingested data with Delta Lake

### ‚ö†Ô∏è **Disclaimer**

This project is **not suitable for production environments**. It is intended for **learning purposes** and for **personal projects** related to Digitraffic data, Docker, Kafka, or PySpark with Delta Lake.
To run a production environment, significant changes would be necessary to the kafka service, implementing multiple brokers and a controller. Same for pyspark, that should be implemented on multiple parallel compute resources

---

## Prerequisites

Ensure you have the following installed on your system:

- **Docker**

---

## Getting Started

### 1. Clone the Repository

```sh
git clone https://github.com/NathansantosIT/digitraffic-data-processing
cd digitraffic-data-processing
```

### 2. Build and Run the Docker Containers

Run the following command from the **main directory** of the project:

```sh
docker-compose up
```

This will start all three services:

- **Kafka Broker** (to manage message queue)
- **Data Fetcher** (retrieves API data and sends to Kafka)
- **PySpark Notebook** (processes the data with Delta Lake)

---

## Running the Data Ingestion Script

Once the containers are up and running, the **data ingestion service** will automatically fetch current train data from the API and publishing it to Kafka.

If you need to manually start the data ingestion service inside the container, use:

```sh
docker exec -it digitraffic_retriever python /app/scr/api_retriever.py
```

---

## Running Jupyter Notebook with Delta

Once the containers are up and running, the **data processing service** will extract kafka data, generate the data quality report and save a standardized delta table.

To open the Jupyter Notebook to read and analyze the data generated by this service, follow:

1. Search for the localhost URL + token at the start of your delta_notebook docker container Logs - It should look like this: http://127.0.0.1:8888/lab?token=token_code
2. Open the `data_analysis.ipynb` notebook.
3. Enjoy your learning trip with Spark and Delta! 

---

## Example outputs 

The three main example outputs can be visualized at the [data_files](https://github.com/NathansantosIT/digitraffic-data-processing/tree/main/data_files) folder.
Although delta format is not the easiest to read, running this project as stated before will open a Jupyter Notebook environment with all necessary dependencies

---

## Project Architecture

The project consists of **three Docker services**:

- ``: Python script fetching data from Digitraffic API and sending to Kafka.
- ``: Apache Kafka broker managing the message queue. The kafka is configured on Apache Kafka Raft (KRaft) mode, which simplifies the architecure by removing its dependency from zookeeper
- ``: Jupyter Notebook running PySpark with Delta Lake for data processing. Similar to the Kafka service, the base image used for this service is a simplified version running on a single core 

### Why PySpark with Delta Lake?

Although this project is currently **designed for batch processing**, using **Delta Lake** allows for an **easy transition to real-time streaming** if needed in the future.
It's also surprisingly difficult to find a simple docker image running a delta pyspark notebook, so this project may serve as a base for diving into Spark

---

## Stopping the Services

To remove all containers, networks, and volumes, use:

```sh
docker-compose down
```

---

## Future Improvements

- **Implement Real-Time Processing**: Modify the python script to fetch data continuously, and the PySpark job to a Spark Streaming continuous data ingestion
- **Enhance Error Handling**: Improve retries and logging for the data ingestion and processing script
- **Testing**: Add testing for python scripts

---

## License

This project is open-source and available for learning purposes. Feel free to modify and extend it! üöÄ
